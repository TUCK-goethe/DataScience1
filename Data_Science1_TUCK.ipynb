{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "#if nltk is not installed uncomment the next line\n",
    "#!{sys.executable} -m pip install nltk\n",
    "import sklearn\n",
    "import sklearn.pipeline as sp\n",
    "import sklearn.datasets as sd\n",
    "import sklearn.ensemble as se\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import sys\n",
    "import random\n",
    "import pandas\n",
    "import re\n",
    "import numpy as np\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import time\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concate the Datasets and create new .csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load all Datasets\n",
    "df1 = pandas.read_csv('train_40k.csv', header = 0, encoding = 'utf-8')\n",
    "df2 = pandas.read_csv('reviews.csv', header = 0, encoding = 'utf-8')\n",
    "df3 = pandas.read_json('Pet_Supplies_5.json', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#drop all columns except from Text and Label\n",
    "df1.drop(df1.columns.difference(['Text', 'Cat1']), 1, inplace=True)\n",
    "df1.columns = ['text','label']\n",
    "df2.drop(df2.columns.difference(['Text']), 1, inplace=True)\n",
    "df3.drop(df3.columns.difference(['reviewText']), 1, inplace=True)\n",
    "\n",
    "#get the first 2300 rows \n",
    "df2 = df2.head(2300)\n",
    "df2.columns = ['text']\n",
    "df2['label'] = 'grocery gourmet food'\n",
    "\n",
    "#get the first 1050 rows\n",
    "df3 = df3.head(1050)\n",
    "df3.columns = ['text']\n",
    "df3['label'] = 'pet supplies'\n",
    "\n",
    "#dataset for plot\n",
    "df4 = pandas.concat([df1, df2, df3])\n",
    "\n",
    "#delete the first 4247 rows with label toys games\n",
    "indexNames_tg = df1[ df1['label'] == 'toys games'].index\n",
    "indexNames_tg = indexNames_tg[:4247]\n",
    "df1.drop(indexNames_tg, inplace=True)\n",
    "\n",
    "#delete the first 3600 rows with label health personal care\n",
    "indexNames_hpc = df1[ df1['label'] == 'health personal care'].index\n",
    "indexNames_hpc = indexNames_hpc[:3600]\n",
    "df1.drop(indexNames_hpc, inplace=True)\n",
    "\n",
    "#concate the Datasets and shuffle the new one\n",
    "df = pandas.concat([df1, df2, df3])\n",
    "df = df.sample(frac=1)\n",
    "#save the Dataset as a csv file\n",
    "df.to_csv('concate_dataset_unclean.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset and Data Quality Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load our merged Dataset\n",
    "df = pandas.read_csv('concate_dataset_unclean.csv', header = 0, encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unbalanced Dataset\n",
    "df4_plot = pandas.DataFrame({'lab':['tg', 'hpc', 'ggf', 'ps', 'b', 'bp'], \\\n",
    "                   'val':[10266, 9772, 5917, 5912, 5846, 5637]})\n",
    "ax = df4_plot.plot.bar(x='lab', y='val', rot=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Completeness\n",
    "value_nan = df[\"text\"].isna().sum().item()\n",
    "value_empty = len(df.index[df['text'] == \"\"].tolist())\n",
    "print('Completeness before Cleaning: ', (1 - (value_nan + value_empty)/len(df))*100,\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Uniqueness\n",
    "duplicateRowsDF = len(df[df.duplicated(['text'])])\n",
    "print('Uniqueness before Cleaning: ', (1 - (duplicateRowsDF / len(df)))*100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Timeliness\n",
    "from datetime import date, datetime, time\n",
    "date_start = date(2020, 6, 3)\n",
    "date_ds1 = date(2020, 4, 2)\n",
    "date_ds2 = date(2017, 5, 1)\n",
    "date_ds3 = date(2016, 4, 26)\n",
    "print('Timeliness Datensatz[3]: ', date_start - date_ds1)\n",
    "print('Timeliness Datensatz[4]: ', date_start - date_ds2)\n",
    "print('Timeliness Datensatz[5]: ', date_start - date_ds3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Validity\n",
    "print('Validity before Cleaning: ', ((2300+1050)/(39489-4247-3600)) * 100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accuracy\n",
    "df2 = df[df['text'].duplicated() == True]\n",
    "text_dup = df2['text'].unique().tolist()\n",
    "counter = 0\n",
    "for i in text_dup:\n",
    "    df_unique = df2.loc[df2['text'] == i]\n",
    "    if len(df_unique['label'].unique().tolist()) > 1:\n",
    "        counter += len(df_unique['label'].unique().tolist())\n",
    "print('Accuracy before Cleaning: ', (1 - (counter / (len(df)))) * 100, '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Consistency\n",
    "print('Consistency before Cleaning: ', (1 - ((2300+1050)/(39489-4247-3600))) * 100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Major Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load our merged Dataset\n",
    "df = pandas.read_csv('concate_dataset_unclean.csv', header = 0, encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#translate the dataset into english\n",
    "#not necessary for our dataset / needs a lot of time\n",
    "#if translate is not installed uncomment the next line\n",
    "#!{sys.executable} -m pip install googletrans\n",
    "import time\n",
    "from googletrans import Translator\n",
    "translator = Translator()\n",
    "t01 = time.time()\n",
    "for i in range(len(df)):\n",
    "    if (translator.detect(df['text'].values[i]).lang) != 'en':\n",
    "        df['text'].values[i] = (translator.translate(df['text'].values[i])).text\n",
    "t02 = time.time()\n",
    "print(t02-t01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shortform in longform and remove Punctuation\n",
    "#Punctuations have low information value\n",
    "#Therefore we removed them\n",
    "\n",
    "t01 = time.time()\n",
    "def f1(text_new):\n",
    "    text_new = re.sub(r\"won\\'t\", \"will not\", text_new)\n",
    "    text_new = re.sub(r\"can\\'t\", \"can not\", text_new)\n",
    "    text_new = re.sub(r\"\\'m\", \" am\", text_new)\n",
    "    text_new = re.sub(r\"\\'re\", \" are\", text_new)\n",
    "    text_new = re.sub(r\"\\'ve\", \" have\", text_new)\n",
    "    text_new = re.sub(r\"n\\'t\", \" not\", text_new)\n",
    "    text_new = re.sub(r\"\\'d\", \" would\", text_new)\n",
    "    text_new = re.sub(r\"\\'ll\", \" will\", text_new)\n",
    "    text_new = re.sub(r\"\\'t\", \" not\", text_new)\n",
    "    text_new = re.sub(r\"\\'s\", \"\", text_new)\n",
    "    text_new = re.sub(r\"\\d+\", \"\", text_new)\n",
    "    return text_new\n",
    "\n",
    "for i in range(0, len(df)):\n",
    "    df['text'].values[i] = df['text'].values[i].translate(str.maketrans('', '', string.punctuation))\n",
    "    df['text'].values[i] = f1(df['text'].values[i])\n",
    "    \n",
    "t02 = time.time()\n",
    "print(t02-t01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lower case\n",
    "#Unfortunately stopword only recognized lower case words\n",
    "#Therefore we had to apply this method before Stopwords\n",
    "t01 = time.time()\n",
    "for i in range(len(df)):\n",
    "    df['text'].values[i] = (df['text'].values[i]).lower()\n",
    "    \n",
    "t02 = time.time()\n",
    "print(t02-t01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#remove StopWords\n",
    "#remove with low information value\n",
    "#if gensim is not installed uncomment the next line\n",
    "#!{sys.executable} -m pip install gensim\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "all_stopwords_gensim = STOPWORDS\n",
    "\n",
    "t01 = time.time()\n",
    "for i in range(len(df)):\n",
    "    text_tokens = word_tokenize(df['text'].values[i])\n",
    "    tokens_without_sw = [word for word in text_tokens if not word in all_stopwords_gensim]\n",
    "    df['text'].values[i] = (\" \").join(tokens_without_sw)\n",
    "t02 = time.time()\n",
    "print(t02-t01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Reference: https://medium.com/@SeoJaeDuk/basic-data-cleaning-engineering-session-twitter-sentiment-data-b9376a91109b\n",
    "#Lemmatization\n",
    "#transforms the word into its word stem\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lmtzr = WordNetLemmatizer()\n",
    "t01 = time.time()\n",
    "\n",
    "df['text'] = df['text'].apply(lambda x: ' '.join([lmtzr.lemmatize(word, 'v') for word in x.split()]))\n",
    "t02 = time.time()\n",
    "print(t02-t01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Reference: https://medium.com/@SeoJaeDuk/basic-data-cleaning-engineering-session-twitter-sentiment-data-b9376a91109b\n",
    "#Stemming\n",
    "#cuts off the end of words. It was way more efficient than Lemmatization.\n",
    "#Even though the functionalities seem similair\n",
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "t01 = time.time()\n",
    "\n",
    "df['text'] = df['text'].apply(lambda x: ' '.join([ps.stem(word) for word in x.split()]))\n",
    "t02 = time.time()\n",
    "print(t02-t01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reference: https://medium.com/@SeoJaeDuk/basic-data-cleaning-engineering-session-twitter-sentiment-data-b9376a91109b\n",
    "#not necessary for our Classification\n",
    "#Parts of Speech Tagging which we did not use for this project\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "df['text_pos'] = df['text'].apply(lambda x: nltk.pos_tag(nltk.word_tokenize(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SARCASM\n",
    "#Here we would implement a NLP method for sarcasm, \n",
    "#but unfortunately we could not find a solution for that\n",
    "#It is probably not possible at this point of time\n",
    "\n",
    "#EMOJI\n",
    "#For our first Twitter Dataset we implemented a Function to translate\n",
    "#emojis to unicode, which we did not need for this Dataset\n",
    "\n",
    "#NORMALIZATION\n",
    "#Another NLP Method one could add in the future is the normalization,\n",
    "#which transforms the most common abbreviations into their written out form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Drop Duplicates, remove Nan and empty strings\n",
    "#Important for Data Quality Dimensions\n",
    "df = df[df.text != \"\"]\n",
    "df.drop(df.columns.difference(['text', 'label']), 1, inplace=True)\n",
    "df = df.replace(np.nan, '', regex=True)\n",
    "df = df.drop_duplicates(subset =\"text\", keep = \"first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#load the Dataset in a csv file\n",
    "df.to_csv('concate_dataset_clean.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the cleaned Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Cleaning with all NLPs\n",
    "df = pandas.read_csv('concate_dataset_clean.csv', header = 0, encoding = 'utf-8')\n",
    "#Without Data Cleaning\n",
    "#df = pandas.read_csv('concate_dataset_unclean.csv', header = 0, encoding = 'utf-8')\n",
    "#Data Cleaning with Punctuation NLP\n",
    "#df = pandas.read_csv('concate_dataset_clean_sz.csv', header = 0, encoding = 'utf-8')\n",
    "#Data Cleaning with Lower Case NLP\n",
    "#df = pandas.read_csv('concate_dataset_clean_lc.csv', header = 0, encoding = 'utf-8')\n",
    "#Data Cleaning with Stopwords NLP\n",
    "#df = pandas.read_csv('concate_dataset_clean_sw.csv', header = 0, encoding = 'utf-8')\n",
    "#Data Cleaning with Lemmatization NLP\n",
    "#df = pandas.read_csv('concate_dataset_clean_lemma.csv', header = 0, encoding = 'utf-8')\n",
    "#Data Cleaning with Stemming NLP\n",
    "#df = pandas.read_csv('concate_dataset_clean_stemming.csv', header = 0, encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Data Quality Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fair balancing\n",
    "df_plot = pandas.DataFrame({'lab':['tg', 'hpc', 'b', 'bp', 'ps', 'ggf'], \\\n",
    "                   'val':[5999, 5996, 5720, 5720, 5863, 5886]})\n",
    "ax = df_plot.plot.bar(x='lab', y='val', rot=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Completeness\n",
    "value_nan = df[\"text\"].isna().sum().item()\n",
    "value_empty = len(df.index[df['text'] == \"\"].tolist())\n",
    "print('Completeness after Data Cleaning: ', (1 - (value_nan + value_empty)/len(df))*100,\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Uniqueness\n",
    "duplicateRowsDF = len(df[df.duplicated(['text'])])\n",
    "print('Uniqueness after Data Cleaning', (1 - (duplicateRowsDF / len(df)))*100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Timeliness\n",
    "from datetime import date, datetime, time\n",
    "date_start = date(2020, 6, 3)\n",
    "date_ds1 = date(2020, 4, 2)\n",
    "date_ds2 = date(2017, 5, 1)\n",
    "date_ds3 = date(2016, 4, 26)\n",
    "print('Timeliness Datensatz[3]: ', date_start - date_ds1)\n",
    "print('Timeliness Datensatz[4]: ', date_start - date_ds2)\n",
    "print('Timeliness Datensatz[5]: ', date_start - date_ds3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Validity\n",
    "print('Validity after Cleaning: ', ((len(df))/(len(df))) * 100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accuracy\n",
    "df2 = df[df['text'].duplicated() == True]\n",
    "text_dup = df2['text'].unique().tolist()\n",
    "counter = 0\n",
    "for i in text_dup:\n",
    "    df_unique = df2.loc[df2['text'] == i]\n",
    "    if len(df_unique['label'].unique().tolist()) > 1:\n",
    "        counter += len(df_unique['label'].unique().tolist())\n",
    "print('Accuracy after Cleaning: ', (1 - (counter / (len(df)))) * 100, '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Consistency\n",
    "print('Consistency after Cleaning: ', ((len(df))/(len(df))) * 100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Fill Train and Testdatasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#drop all columns except from text and label and delete all nan\n",
    "df.drop(df.columns.difference(['text', 'label']), 1, inplace=True)\n",
    "df = df.replace(np.nan, '', regex=True)\n",
    "#list for features\n",
    "data_text = []\n",
    "#list for labels\n",
    "data_labels = []\n",
    "\n",
    "for i in range(0, len(df)):                                  \n",
    "    data_text.append(df['text'].values[i])\n",
    "    data_labels.append(df['label'].values[i])\n",
    "    \n",
    "#fill the train and datasets\n",
    "train_X = data_text[:round((0.8*len(data_text)))]\n",
    "train_Y = data_labels[:round((0.8*len(data_labels)))]\n",
    "test_X = data_text[round((0.8*len(data_text))):]\n",
    "test_Y = data_labels[round((0.8*len(data_labels))):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorize Train and Test Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(lowercase=False)\n",
    "trainset_2_vectors = tfidf.fit_transform(train_X)\n",
    "testset_2_vectors = tfidf.transform(test_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Fit Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "t0 = time.time()\n",
    "#Initialize and train GradientBoostingClassifier with 100 estimators (GBR(100))\n",
    "gbc = se.GradientBoostingClassifier(n_estimators=100, learning_rate = 0.1,\n",
    "     max_depth=5, random_state=0).fit(trainset_2_vectors, train_Y)\n",
    "y_pred_gbc = gbc.predict(testset_2_vectors)\n",
    "t1 = time.time()\n",
    "print('Time for GradientBoostingClassifier: ', t1-t0)\n",
    "\n",
    "t01 = time.time()\n",
    "#Initialize and train Random Forest with 100 estimators (RFC(100))\n",
    "random_forest_algo = se.RandomForestClassifier(n_estimators=\\\n",
    "                        100).fit(trainset_2_vectors, train_Y)\n",
    "y2_pred = random_forest_algo.predict(testset_2_vectors)\n",
    "t11 = time.time()\n",
    "print('Time for Random Forest: ', t11-t01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Accuracy from each Classifier\n",
    "print(\"Accuracy from GradientBoostingClassifier: \", sklearn.metrics.f1_score(test_Y, y_pred_gbc, average='weighted'))\n",
    "print(\"Accuracy from Random Forest: \", sklearn.metrics.f1_score(test_Y, y2_pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explanation with LIME in AIX360"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#LIME\n",
    "#With Lime we can explain why the class was classified\n",
    "#create a pipeline for each of the Classifier\n",
    "pipeline_rf = sklearn.pipeline.make_pipeline(tfidf, random_forest_algo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#if aix360 is not installed uncomment the next line\n",
    "#!{sys.executable} -m pip install aix360\n",
    "from aix360.algorithms.lime import LimeTextExplainer\n",
    "#Explain text classifiers with LimeTextExplainer\n",
    "limeexplainer = LimeTextExplainer(class_names = sorted(df.label.unique()))\n",
    "x = random.randint(0,(len(train_X)))\n",
    "#explanation of the assignment of a specific data point for Random Forest\n",
    "ex_rf = limeexplainer.explain_instance(train_X[x], pipeline_rf.predict_proba, top_labels=7, num_features=5)\n",
    "ex_rf.show_in_notebook(text = train_X[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
